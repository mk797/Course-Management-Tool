{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3 Overview\n",
    "\n",
    "\n",
    "In this assignment, we will study neural IR, including word embedding and LLM.\n",
    "\n",
    "We will reuse the same Yelp dataset and refer to each individual user review as a **document**. You should reuse your JSON parser in this assignment.\n",
    "\n",
    "The same pre-processing steps you have developed in HW1 will be used in this assignment, i.e., tokenization, stemming, normalization and stopword removal. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding (50 points)\n",
    "\n",
    "Use [genism](https://radimrehurek.com/gensim/models/word2vec.html) library, download a pre-trained model or train a word2vec model (10 epochs) using the review data. Then use the model to get the vector representations of the document and query.\n",
    "\n",
    "Hint: You can average the embedding of the terms to get the vector representation \n",
    "for a document.\n",
    "\n",
    "Use the following 10 queries (same as the ones in HW1) and retrieve the top 3 documents for each query based on cosine similarity:\n",
    "\n",
    "\tgeneral chicken\n",
    "\tfried chicken\n",
    "\tBBQ sandwiches\n",
    "\tmashed potatoes\n",
    "\tGrilled Shrimp Salad\n",
    "\tlamb Shank\n",
    "\tPepperoni pizza\n",
    "\tbrussel sprout salad\n",
    "\tFRIENDLY STAFF\n",
    "\tGrilled Cheese\n",
    "\n",
    "Note that the training does not require GPU -- moderate CPU is good enough.  If your computation power is limited, i.e., limited memory or cpu, you can choose to train the model on partial data, e.g., 50% or fewer review data. Please document the corresponding training detail in your report.\n",
    "\n",
    "**What to submit**:\n",
    "\n",
    "1. Paste your implementation of training document representation and cosine similarity calculation. Report the training time of the word embedding model. (15 points)\n",
    "2. For the top 3 documents of each query, print the document and its cosine similarity score. (15 points)\n",
    "3. For the first three queries, analyze the relation between relevance and cosine similarity score: is a high score document more relevant to the query? (10 points)\n",
    "4. Are the documents more relevant than documents retrieved by TF-IDF in Homework 1? Why or why not? Compare and discuss the results. (10 points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Representation (40 points)\n",
    "We further investigate a document representation method, [doc2vec](https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.htm). Train a doc2vec model (10 epochs)  using the review data or download a pre-trained model. Then use the model to get the vector representations of the document and query. Use the same 10 queries and retrieve the top 3 documents for each query based on cosine similarity.\n",
    "\n",
    "**What to submit**:\n",
    "\n",
    "1. Paste your implementation of training, document representation and cosine similarity calculation. Report the training time of document embedding model. (15 points)\n",
    "2. For the top 3 documents of each query, print the document and its cosine similarity score. (15 points)\n",
    "3. For the first three queries, analyze the relation between relevance and cosine similarity score.(10 points)\n",
    "4. Compare and discuss the results with TF-IDF and word2vec: Is it harder to train? Does doc2vec perform better than the two models we investigated?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credits (20pts)\n",
    "\n",
    "Retrieval with Large Language Model: query a large language model of your choice (close-sourced like ChatGPT or Claude, open-sourced like Llama) with the same review data and 10 queries.  Prompt LLM the way you like, and an example can be found in [https://arxiv.org/abs/2304.09542]. You may consider pointwise, pairwise, and/or listwise prompts. Retrieve the top 3 documents for each query. Are the documents more relevant than documents retrieved by word2vec and doc2vec?\n",
    "\n",
    "\n",
    "# Submission\n",
    "\n",
    "This assignment has in total 100 points. The deadline is May 28th 23:59 PDT. You should submit your report in **PDF** using the homework latex template, and submit your code (notebook). "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
